{
    "buggy_classes": [
        "org.apache.commons.math3.optimization.direct.CMAESOptimizer"
    ],
    "buggy_methods": [
        {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::encode(double[])",
            "method_code": "```java\n        public double[] encode(final double[] x) {\n            if (boundaries == null) {\n                return x;\n            }\n            double[] res = new double[x.length];\n            for (int i = 0; i < x.length; i++) {\n                double diff = boundaries[1][i] - boundaries[0][i];\n                res[i] = (x[i] - boundaries[0][i]) / diff;\n            }\n            return res;\n        }\n\n```",
            "method_doc": " @param x Original objective variables. @return the normalized objective variables. ",
            "class_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer",
            "class_doc": " <p>An implementation of the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for non-linear, non-convex, non-smooth, global function minimization. The CMA-Evolution Strategy (CMA-ES) is a reliable stochastic optimization method which should be applied if derivative-based methods, e.g. quasi-Newton BFGS or conjugate gradient, fail due to a rugged search landscape (e.g. noise, local optima, outlier, etc.) of the objective function. Like a quasi-Newton method, the CMA-ES learns and applies a variable metric on the underlying search space. Unlike a quasi-Newton method, the CMA-ES neither estimates nor uses gradients, making it considerably more reliable in terms of finding a good, or even close to optimal, solution.</p>  <p>In general, on smooth objective functions the CMA-ES is roughly ten times slower than BFGS (counting objective function evaluations, no gradients provided). For up to <math>N=10</math> variables also the derivative-free simplex direct search method (Nelder and Mead) can be faster, but it is far less reliable than CMA-ES.</p>  <p>The CMA-ES is particularly well suited for non-separable and/or badly conditioned problems. To observe the advantage of CMA compared to a conventional evolution strategy, it will usually take about <math>30 N</math> function evaluations. On difficult problems the complete optimization (a single run) is expected to take <em>roughly</em> between <math>30 N</math> and <math>300 N<sup>2</sup></math> function evaluations.</p>  <p>This implementation is translated and adapted from the Matlab version of the CMA-ES algorithm as implemented in module {@code cmaes.m} version 3.51.</p>  For more information, please refer to the following links: <ul> <li><a href=\"http://www.lri.fr/~hansen/cmaes.m\">Matlab code</a></li> <li><a href=\"http://www.lri.fr/~hansen/cmaesintro.html\">Introduction to CMA-ES</a></li> <li><a href=\"http://en.wikipedia.org/wiki/CMA-ES\">Wikipedia</a></li> </ul>  When simple constraints (boundaries) are used, care must be taken that the difference between the upper and lower bounds does not overflow; should it be the case, a {@link NumberIsTooLargeException} will be thrown by the {@link BaseAbstractMultivariateSimpleBoundsOptimizer#optimize(int, MultivariateFunction,GoalType,double[],double[],double[]) optimize} method.  @version $Id$ @since 3.0 ",
            "test_failure_causes": "Based on my analysis of the failed test case, stack trace, and test behavior, here's my assessment:\n\n## Common Patterns and Similarities\n\n1. **Exception Mismatch**: The test expects `NumberIsTooLargeException` but receives `MathIllegalStateException`\n2. **Boundary Overflow Scenario**: All failures occur when dealing with extremely large boundary ranges that approach or exceed `Double.MAX_VALUE`\n3. **NaN Detection**: The custom fitness function explicitly checks for `NaN` values and throws `MathIllegalStateException` when detected\n4. **Normalization Issue**: The test comments indicate that boundary differences are used for normalization, and overflow during this process produces `NaN`\n\n## Recommended Production Code Defect\n\nThe likely defect in the `CMAESOptimizer` production code is:\n\n**Insufficient boundary range validation before normalization calculations**\n\nThe optimizer appears to be attempting to normalize variables using the difference between upper and lower bounds without first checking if this difference would cause numerical overflow. When the boundary range (upper - lower) exceeds `Double.MAX_VALUE`, the normalization calculation produces `NaN` values, which then propagate through the optimization process.\n\nSpecifically, the defect likely occurs in the boundary handling logic where:\n1. The code calculates `range = upperBound[i] - lowerBound[i]` for normalization\n2. When `upperBound[i]` and `lowerBound[i]` are both extremely large values (close to `Double.MAX_VALUE`), their difference can overflow and produce `NaN`\n3. This `NaN` value then gets passed to the fitness function\n4. The fitness function detects the `NaN` and throws `MathIllegalStateException` instead of the expected `NumberIsTooLargeException`\n\nThe production code should validate boundary ranges before performing normalization calculations and throw `NumberIsTooLargeException` when the range is too large to handle numerically, rather than allowing the overflow to occur and produce `NaN` values.",
            "test_suite": "org.apache.commons.math3.optimization.direct.CMAESOptimizerTest",
            "score": 9,
            "reason": "#9# This method is highly suspicious because it directly performs the boundary range calculation (boundaries[1][i] - boundaries[0][i]) without any overflow checking. When the difference between upper and lower bounds exceeds Double.MAX_VALUE, this subtraction will produce NaN, which then causes division by NaN and propagates through the optimization process, ultimately triggering the MathIllegalStateException instead of the expected NumberIsTooLargeException. The method should validate the boundary range before performing the normalization calculation."
        },
        {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer::checkParameters()",
            "method_code": "```java\n    private void checkParameters() {\n        if (!(!(false))) {\n            System.out.println(\"log\");\n        }\n        final double[] TransVarFinalInit = getStartPoint();\n        final double[] TransVarLowerBound = getLowerBound();\n        final double[] TransVarUB2 = getUpperBound();\n\n        // Checks whether there is at least one finite bound value.\n        boolean TransVarHasLowerBounds = false;\n        int TransVarSum = 0;\n        while (!(!(TransVarSum < TransVarLowerBound.length))) {\n            if (!(!(false))) {\n                System.out.println(\"log\");\n            }\n            if (!(!(!Double.isInfinite(TransVarLowerBound[TransVarSum]) || !Double.isInfinite(TransVarUB2[TransVarSum])))) {\n                if (!(!(false))) {\n                    System.out.println(\"log\");\n                }\n                TransVarHasLowerBounds = true;\n                break;\n            }\n            TransVarSum++;\n        }\n        // Checks whether there is at least one infinite bound value.\n        boolean TransVarHasUpperBounds = false;\n        if (!(!(TransVarHasLowerBounds))) {\n            int TransVarInit = 0;\n            while (!(!(TransVarInit < TransVarLowerBound.length))) {\n                if (!(!(Double.isInfinite(TransVarLowerBound[TransVarInit])\n                        || Double.isInfinite(TransVarUB2[TransVarInit])))) {\n                    TransVarHasUpperBounds = true;\n                    break;\n                }\n                TransVarInit++;\n            }\n\n            if (!(!(TransVarHasUpperBounds))) {\n                // If there is at least one finite bound, none can be infinite,\n                // because mixed cases are not supported by the current code.\n                throw new MathUnsupportedOperationException();\n            } else {\n                // Convert API to internal handling of boundaries.\n                boundaries = new double[2][];\n                boundaries[0] = TransVarLowerBound;\n                boundaries[1] = TransVarUB2;\n\n                // Abort early if the normalization will overflow (cf. \"encode\" method).\n                    }\n        } else {\n            // Convert API to internal handling of boundaries.\n            boundaries = null;\n        }\n\n        if (!(!(inputSigma != null))) {\n            if (!(!(inputSigma.length != TransVarFinalInit.length))) {\n                throw new DimensionMismatchException(inputSigma.length, TransVarFinalInit.length);\n            }\n            int TransVarJ = 0;\n            while (!(!(TransVarJ < TransVarFinalInit.length))) {\n                if (!(!(inputSigma[TransVarJ] < 0))) {\n                    throw new NotPositiveException(inputSigma[TransVarJ]);\n                }\n                if (!(!(boundaries != null))) {\n                    if (!(!(inputSigma[TransVarJ] > boundaries[1][TransVarJ] - boundaries[0][TransVarJ]))) {\n                        throw new OutOfRangeException(inputSigma[TransVarJ], 0,\n                                boundaries[1][TransVarJ] - boundaries[0][TransVarJ]);\n                    }\n                }\n                TransVarJ++;\n            }\n        }\n    }\n\n```",
            "method_doc": " Checks dimensions and values of boundaries and inputSigma if defined. ",
            "class_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer",
            "class_doc": " <p>An implementation of the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for non-linear, non-convex, non-smooth, global function minimization. The CMA-Evolution Strategy (CMA-ES) is a reliable stochastic optimization method which should be applied if derivative-based methods, e.g. quasi-Newton BFGS or conjugate gradient, fail due to a rugged search landscape (e.g. noise, local optima, outlier, etc.) of the objective function. Like a quasi-Newton method, the CMA-ES learns and applies a variable metric on the underlying search space. Unlike a quasi-Newton method, the CMA-ES neither estimates nor uses gradients, making it considerably more reliable in terms of finding a good, or even close to optimal, solution.</p>  <p>In general, on smooth objective functions the CMA-ES is roughly ten times slower than BFGS (counting objective function evaluations, no gradients provided). For up to <math>N=10</math> variables also the derivative-free simplex direct search method (Nelder and Mead) can be faster, but it is far less reliable than CMA-ES.</p>  <p>The CMA-ES is particularly well suited for non-separable and/or badly conditioned problems. To observe the advantage of CMA compared to a conventional evolution strategy, it will usually take about <math>30 N</math> function evaluations. On difficult problems the complete optimization (a single run) is expected to take <em>roughly</em> between <math>30 N</math> and <math>300 N<sup>2</sup></math> function evaluations.</p>  <p>This implementation is translated and adapted from the Matlab version of the CMA-ES algorithm as implemented in module {@code cmaes.m} version 3.51.</p>  For more information, please refer to the following links: <ul> <li><a href=\"http://www.lri.fr/~hansen/cmaes.m\">Matlab code</a></li> <li><a href=\"http://www.lri.fr/~hansen/cmaesintro.html\">Introduction to CMA-ES</a></li> <li><a href=\"http://en.wikipedia.org/wiki/CMA-ES\">Wikipedia</a></li> </ul>  When simple constraints (boundaries) are used, care must be taken that the difference between the upper and lower bounds does not overflow; should it be the case, a {@link NumberIsTooLargeException} will be thrown by the {@link BaseAbstractMultivariateSimpleBoundsOptimizer#optimize(int, MultivariateFunction,GoalType,double[],double[],double[]) optimize} method.  @version $Id$ @since 3.0 ",
            "test_failure_causes": "Based on my analysis of the failed test case, stack trace, and test behavior, here's my assessment:\n\n## Common Patterns and Similarities\n\n1. **Exception Mismatch**: The test expects `NumberIsTooLargeException` but receives `MathIllegalStateException`\n2. **Boundary Overflow Scenario**: All failures occur when dealing with extremely large boundary ranges that approach or exceed `Double.MAX_VALUE`\n3. **NaN Detection**: The custom fitness function explicitly checks for `NaN` values and throws `MathIllegalStateException` when detected\n4. **Normalization Issue**: The test comments indicate that boundary differences are used for normalization, and overflow during this process produces `NaN`\n\n## Recommended Production Code Defect\n\nThe likely defect in the `CMAESOptimizer` production code is:\n\n**Insufficient boundary range validation before normalization calculations**\n\nThe optimizer appears to be attempting to normalize variables using the difference between upper and lower bounds without first checking if this difference would cause numerical overflow. When the boundary range (upper - lower) exceeds `Double.MAX_VALUE`, the normalization calculation produces `NaN` values, which then propagate through the optimization process.\n\nSpecifically, the defect likely occurs in the boundary handling logic where:\n1. The code calculates `range = upperBound[i] - lowerBound[i]` for normalization\n2. When `upperBound[i]` and `lowerBound[i]` are both extremely large values (close to `Double.MAX_VALUE`), their difference can overflow and produce `NaN`\n3. This `NaN` value then gets passed to the fitness function\n4. The fitness function detects the `NaN` and throws `MathIllegalStateException` instead of the expected `NumberIsTooLargeException`\n\nThe production code should validate boundary ranges before performing normalization calculations and throw `NumberIsTooLargeException` when the range is too large to handle numerically, rather than allowing the overflow to occur and produce `NaN` values.",
            "test_suite": "org.apache.commons.math3.optimization.direct.CMAESOptimizerTest",
            "score": 9,
            "reason": "#9# This method is highly suspicious because it contains boundary validation logic but fails to properly check for numerical overflow when calculating boundary ranges. The test expects NumberIsTooLargeException when boundary ranges are too large, but the current code allows boundary range calculations (boundaries[1][TransVarJ] - boundaries[0][TransVarJ]) that can overflow and produce NaN values, which then propagate through the optimization process and cause MathIllegalStateException instead of the expected exception."
        },
        {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer::doOptimize()",
            "method_code": "```java\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = fitfun.encode(getStartPoint());\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum = new PointValuePair(getStartPoint(),\n                isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n            for (iterations = 1; iterations <= maxIterations; iterations++) {\n                // Generate and evaluate lambda offspring\n                RealMatrix arz = randn1(dimension, lambda);\n                RealMatrix arx = zeros(dimension, lambda);\n                double[] fitness = new double[lambda];\n                // generate random offspring\n                for (int k = 0; k < lambda; k++) {\n                    RealMatrix arxk = null;\n                    for (int i = 0; i < checkFeasableCount+1; i++) {\n                        if (diagonalOnly <= 0) {\n                            arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                    .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                        } else {\n                            arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                    .scalarMultiply(sigma));\n                        }\n                        if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                            break;\n                        }\n                        // regenerate random arguments for row\n                        arz.setColumn(k, randn(dimension));\n                    }\n                    copyColumn(arxk, 0, arx, k);\n                    try {\n                        fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                    } catch (TooManyEvaluationsException e) {\n                        break generationLoop;\n                    }\n                }\n                // Sort by fitness and compute weighted mean into xmean\n                int[] arindex = sortedIndices(fitness);\n                // Calculate new xmean, this is selection and recombination\n                RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n                RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n                xmean = bestArx.multiply(weights);\n                RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n                RealMatrix zmean = bestArz.multiply(weights);\n                boolean hsig = updateEvolutionPaths(zmean, xold);\n                if (diagonalOnly <= 0) {\n                    updateCovariance(hsig, bestArx, arz, arindex, xold);\n                } else {\n                    updateCovarianceDiagonalOnly(hsig, bestArz, xold);\n                }\n                // Adapt step size sigma - Eq. (5)\n                sigma *= Math.exp(Math.min(1.0,(normps/chiN - 1.)*cs/damps));\n                double bestFitness = fitness[arindex[0]];\n                double worstFitness = fitness[arindex[arindex.length-1]];\n                if (bestValue > bestFitness) {\n                    bestValue = bestFitness;\n                    lastResult = optimum;\n                    optimum = new PointValuePair(\n                            fitfun.repairAndDecode(bestArx.getColumn(0)),\n                            isMinimize ? bestFitness : -bestFitness);\n                    if (getConvergenceChecker() != null && lastResult != null) {\n                        if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                            break generationLoop;\n                        }\n                    }\n                }\n                // handle termination criteria\n                // Break, if fitness is good enough\n                if (stopFitness != 0) { // only if stopFitness is defined\n                    if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                        break generationLoop;\n                    }\n                }\n                double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n                double[] pcCol = pc.getColumn(0);\n                for (int i = 0; i < dimension; i++) {\n                    if (sigma*(Math.max(Math.abs(pcCol[i]), sqrtDiagC[i])) > stopTolX) {\n                        break;\n                    }\n                    if (i >= dimension-1) {\n                        break generationLoop;\n                    }\n                }\n                for (int i = 0; i < dimension; i++) {\n                    if (sigma*sqrtDiagC[i] > stopTolUpX) {\n                        break generationLoop;\n                    }\n                }\n                double historyBest = min(fitnessHistory);\n                double historyWorst = max(fitnessHistory);\n                if (iterations > 2 && Math.max(historyWorst, worstFitness) -\n                        Math.min(historyBest, bestFitness) < stopTolFun) {\n                    break generationLoop;\n                }\n                if (iterations > fitnessHistory.length &&\n                        historyWorst-historyBest < stopTolHistFun) {\n                    break generationLoop;\n                }\n                // condition number of the covariance matrix exceeds 1e14\n                if (max(diagD)/min(diagD) > 1e7) {\n                    break generationLoop;\n                }\n                // user defined termination\n                if (getConvergenceChecker() != null) {\n                    PointValuePair current =\n                        new PointValuePair(bestArx.getColumn(0),\n                                isMinimize ? bestFitness : -bestFitness);\n                    if (lastResult != null &&\n                        getConvergenceChecker().converged(iterations, current, lastResult)) {\n                        break generationLoop;\n                    }\n                    lastResult = current;\n                }\n                // Adjust step size in case of equal function values (flat fitness)\n                if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                    sigma = sigma * Math.exp(0.2+cs/damps);\n                }\n                if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                        Math.min(historyBest, bestFitness) == 0) {\n                    sigma = sigma * Math.exp(0.2+cs/damps);\n                }\n                // store best in history\n                push(fitnessHistory,bestFitness);\n                fitfun.setValueRange(worstFitness-bestFitness);\n                if (generateStatistics) {\n                    statisticsSigmaHistory.add(sigma);\n                    statisticsFitnessHistory.add(bestFitness);\n                    statisticsMeanHistory.add(xmean.transpose());\n                    statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n                }\n            }\n        return optimum;\n    }\n\n```",
            "method_doc": "{@inheritDoc} */",
            "class_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer",
            "class_doc": " <p>An implementation of the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for non-linear, non-convex, non-smooth, global function minimization. The CMA-Evolution Strategy (CMA-ES) is a reliable stochastic optimization method which should be applied if derivative-based methods, e.g. quasi-Newton BFGS or conjugate gradient, fail due to a rugged search landscape (e.g. noise, local optima, outlier, etc.) of the objective function. Like a quasi-Newton method, the CMA-ES learns and applies a variable metric on the underlying search space. Unlike a quasi-Newton method, the CMA-ES neither estimates nor uses gradients, making it considerably more reliable in terms of finding a good, or even close to optimal, solution.</p>  <p>In general, on smooth objective functions the CMA-ES is roughly ten times slower than BFGS (counting objective function evaluations, no gradients provided). For up to <math>N=10</math> variables also the derivative-free simplex direct search method (Nelder and Mead) can be faster, but it is far less reliable than CMA-ES.</p>  <p>The CMA-ES is particularly well suited for non-separable and/or badly conditioned problems. To observe the advantage of CMA compared to a conventional evolution strategy, it will usually take about <math>30 N</math> function evaluations. On difficult problems the complete optimization (a single run) is expected to take <em>roughly</em> between <math>30 N</math> and <math>300 N<sup>2</sup></math> function evaluations.</p>  <p>This implementation is translated and adapted from the Matlab version of the CMA-ES algorithm as implemented in module {@code cmaes.m} version 3.51.</p>  For more information, please refer to the following links: <ul> <li><a href=\"http://www.lri.fr/~hansen/cmaes.m\">Matlab code</a></li> <li><a href=\"http://www.lri.fr/~hansen/cmaesintro.html\">Introduction to CMA-ES</a></li> <li><a href=\"http://en.wikipedia.org/wiki/CMA-ES\">Wikipedia</a></li> </ul>  When simple constraints (boundaries) are used, care must be taken that the difference between the upper and lower bounds does not overflow; should it be the case, a {@link NumberIsTooLargeException} will be thrown by the {@link BaseAbstractMultivariateSimpleBoundsOptimizer#optimize(int, MultivariateFunction,GoalType,double[],double[],double[]) optimize} method.  @version $Id$ @since 3.0 ",
            "test_failure_causes": "Based on my analysis of the failed test case, stack trace, and test behavior, here's my assessment:\n\n## Common Patterns and Similarities\n\n1. **Exception Mismatch**: The test expects `NumberIsTooLargeException` but receives `MathIllegalStateException`\n2. **Boundary Overflow Scenario**: All failures occur when dealing with extremely large boundary ranges that approach or exceed `Double.MAX_VALUE`\n3. **NaN Detection**: The custom fitness function explicitly checks for `NaN` values and throws `MathIllegalStateException` when detected\n4. **Normalization Issue**: The test comments indicate that boundary differences are used for normalization, and overflow during this process produces `NaN`\n\n## Recommended Production Code Defect\n\nThe likely defect in the `CMAESOptimizer` production code is:\n\n**Insufficient boundary range validation before normalization calculations**\n\nThe optimizer appears to be attempting to normalize variables using the difference between upper and lower bounds without first checking if this difference would cause numerical overflow. When the boundary range (upper - lower) exceeds `Double.MAX_VALUE`, the normalization calculation produces `NaN` values, which then propagate through the optimization process.\n\nSpecifically, the defect likely occurs in the boundary handling logic where:\n1. The code calculates `range = upperBound[i] - lowerBound[i]` for normalization\n2. When `upperBound[i]` and `lowerBound[i]` are both extremely large values (close to `Double.MAX_VALUE`), their difference can overflow and produce `NaN`\n3. This `NaN` value then gets passed to the fitness function\n4. The fitness function detects the `NaN` and throws `MathIllegalStateException` instead of the expected `NumberIsTooLargeException`\n\nThe production code should validate boundary ranges before performing normalization calculations and throw `NumberIsTooLargeException` when the range is too large to handle numerically, rather than allowing the overflow to occur and produce `NaN` values.",
            "test_suite": "org.apache.commons.math3.optimization.direct.CMAESOptimizerTest",
            "score": 8,
            "reason": "#8# The doOptimize() method is very likely to be the best location to fix because it contains the main optimization loop where boundary handling occurs. The test failure shows that when extremely large boundary ranges are used, NaN values are generated during the optimization process and passed to the fitness function, which then throws MathIllegalStateException instead of the expected NumberIsTooLargeException. The issue likely occurs in the boundary normalization calculations within the optimization algorithm, particularly when generating candidate solutions or handling boundary constraints, which should validate boundary ranges before numerical operations to prevent overflow and NaN generation."
        },
        {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::decode(double[])",
            "method_code": "```java\n        public double[] decode(final double[] x) {\n            if (boundaries == null) {\n                return x;\n            }\n            double[] res = new double[x.length];\n            for (int i = 0; i < x.length; i++) {\n                double diff = boundaries[1][i] - boundaries[0][i];\n                res[i] = diff * x[i] + boundaries[0][i];\n            }\n            return res;\n        }\n\n```",
            "method_doc": " @param x Normalized objective variables. @return the original objective variables. ",
            "class_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer",
            "class_doc": " <p>An implementation of the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for non-linear, non-convex, non-smooth, global function minimization. The CMA-Evolution Strategy (CMA-ES) is a reliable stochastic optimization method which should be applied if derivative-based methods, e.g. quasi-Newton BFGS or conjugate gradient, fail due to a rugged search landscape (e.g. noise, local optima, outlier, etc.) of the objective function. Like a quasi-Newton method, the CMA-ES learns and applies a variable metric on the underlying search space. Unlike a quasi-Newton method, the CMA-ES neither estimates nor uses gradients, making it considerably more reliable in terms of finding a good, or even close to optimal, solution.</p>  <p>In general, on smooth objective functions the CMA-ES is roughly ten times slower than BFGS (counting objective function evaluations, no gradients provided). For up to <math>N=10</math> variables also the derivative-free simplex direct search method (Nelder and Mead) can be faster, but it is far less reliable than CMA-ES.</p>  <p>The CMA-ES is particularly well suited for non-separable and/or badly conditioned problems. To observe the advantage of CMA compared to a conventional evolution strategy, it will usually take about <math>30 N</math> function evaluations. On difficult problems the complete optimization (a single run) is expected to take <em>roughly</em> between <math>30 N</math> and <math>300 N<sup>2</sup></math> function evaluations.</p>  <p>This implementation is translated and adapted from the Matlab version of the CMA-ES algorithm as implemented in module {@code cmaes.m} version 3.51.</p>  For more information, please refer to the following links: <ul> <li><a href=\"http://www.lri.fr/~hansen/cmaes.m\">Matlab code</a></li> <li><a href=\"http://www.lri.fr/~hansen/cmaesintro.html\">Introduction to CMA-ES</a></li> <li><a href=\"http://en.wikipedia.org/wiki/CMA-ES\">Wikipedia</a></li> </ul>  When simple constraints (boundaries) are used, care must be taken that the difference between the upper and lower bounds does not overflow; should it be the case, a {@link NumberIsTooLargeException} will be thrown by the {@link BaseAbstractMultivariateSimpleBoundsOptimizer#optimize(int, MultivariateFunction,GoalType,double[],double[],double[]) optimize} method.  @version $Id$ @since 3.0 ",
            "test_failure_causes": "Based on my analysis of the failed test case, stack trace, and test behavior, here's my assessment:\n\n## Common Patterns and Similarities\n\n1. **Exception Mismatch**: The test expects `NumberIsTooLargeException` but receives `MathIllegalStateException`\n2. **Boundary Overflow Scenario**: All failures occur when dealing with extremely large boundary ranges that approach or exceed `Double.MAX_VALUE`\n3. **NaN Detection**: The custom fitness function explicitly checks for `NaN` values and throws `MathIllegalStateException` when detected\n4. **Normalization Issue**: The test comments indicate that boundary differences are used for normalization, and overflow during this process produces `NaN`\n\n## Recommended Production Code Defect\n\nThe likely defect in the `CMAESOptimizer` production code is:\n\n**Insufficient boundary range validation before normalization calculations**\n\nThe optimizer appears to be attempting to normalize variables using the difference between upper and lower bounds without first checking if this difference would cause numerical overflow. When the boundary range (upper - lower) exceeds `Double.MAX_VALUE`, the normalization calculation produces `NaN` values, which then propagate through the optimization process.\n\nSpecifically, the defect likely occurs in the boundary handling logic where:\n1. The code calculates `range = upperBound[i] - lowerBound[i]` for normalization\n2. When `upperBound[i]` and `lowerBound[i]` are both extremely large values (close to `Double.MAX_VALUE`), their difference can overflow and produce `NaN`\n3. This `NaN` value then gets passed to the fitness function\n4. The fitness function detects the `NaN` and throws `MathIllegalStateException` instead of the expected `NumberIsTooLargeException`\n\nThe production code should validate boundary ranges before performing normalization calculations and throw `NumberIsTooLargeException` when the range is too large to handle numerically, rather than allowing the overflow to occur and produce `NaN` values.",
            "test_suite": "org.apache.commons.math3.optimization.direct.CMAESOptimizerTest",
            "score": 8,
            "reason": "#8# This method is very likely to be the best location that needs to be fixed because it performs the critical boundary range calculation `boundaries[1][i] - boundaries[0][i]` without any overflow checking. When the upper and lower bounds are both extremely large values close to Double.MAX_VALUE, their difference can overflow and produce NaN, which then propagates through the optimization process and causes the MathIllegalStateException instead of the expected NumberIsTooLargeException."
        },
        {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::value(double[])",
            "method_code": "```java\n        public double value(final double[] point) {\n            double value;\n            if (boundaries != null && isRepairMode) {\n                double[] repaired = repair(point);\n                value = CMAESOptimizer.this\n                        .computeObjectiveValue(decode(repaired)) +\n                        penalty(point, repaired);\n            } else {\n                value = CMAESOptimizer.this\n                        .computeObjectiveValue(decode(point));\n            }\n            return isMinimize ? value : -value;\n        }\n\n```",
            "method_doc": " @param point Normalized objective variables. @return the objective value + penalty for violated bounds. ",
            "class_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer",
            "class_doc": " <p>An implementation of the active Covariance Matrix Adaptation Evolution Strategy (CMA-ES) for non-linear, non-convex, non-smooth, global function minimization. The CMA-Evolution Strategy (CMA-ES) is a reliable stochastic optimization method which should be applied if derivative-based methods, e.g. quasi-Newton BFGS or conjugate gradient, fail due to a rugged search landscape (e.g. noise, local optima, outlier, etc.) of the objective function. Like a quasi-Newton method, the CMA-ES learns and applies a variable metric on the underlying search space. Unlike a quasi-Newton method, the CMA-ES neither estimates nor uses gradients, making it considerably more reliable in terms of finding a good, or even close to optimal, solution.</p>  <p>In general, on smooth objective functions the CMA-ES is roughly ten times slower than BFGS (counting objective function evaluations, no gradients provided). For up to <math>N=10</math> variables also the derivative-free simplex direct search method (Nelder and Mead) can be faster, but it is far less reliable than CMA-ES.</p>  <p>The CMA-ES is particularly well suited for non-separable and/or badly conditioned problems. To observe the advantage of CMA compared to a conventional evolution strategy, it will usually take about <math>30 N</math> function evaluations. On difficult problems the complete optimization (a single run) is expected to take <em>roughly</em> between <math>30 N</math> and <math>300 N<sup>2</sup></math> function evaluations.</p>  <p>This implementation is translated and adapted from the Matlab version of the CMA-ES algorithm as implemented in module {@code cmaes.m} version 3.51.</p>  For more information, please refer to the following links: <ul> <li><a href=\"http://www.lri.fr/~hansen/cmaes.m\">Matlab code</a></li> <li><a href=\"http://www.lri.fr/~hansen/cmaesintro.html\">Introduction to CMA-ES</a></li> <li><a href=\"http://en.wikipedia.org/wiki/CMA-ES\">Wikipedia</a></li> </ul>  When simple constraints (boundaries) are used, care must be taken that the difference between the upper and lower bounds does not overflow; should it be the case, a {@link NumberIsTooLargeException} will be thrown by the {@link BaseAbstractMultivariateSimpleBoundsOptimizer#optimize(int, MultivariateFunction,GoalType,double[],double[],double[]) optimize} method.  @version $Id$ @since 3.0 ",
            "test_failure_causes": "Based on my analysis of the failed test case, stack trace, and test behavior, here's my assessment:\n\n## Common Patterns and Similarities\n\n1. **Exception Mismatch**: The test expects `NumberIsTooLargeException` but receives `MathIllegalStateException`\n2. **Boundary Overflow Scenario**: All failures occur when dealing with extremely large boundary ranges that approach or exceed `Double.MAX_VALUE`\n3. **NaN Detection**: The custom fitness function explicitly checks for `NaN` values and throws `MathIllegalStateException` when detected\n4. **Normalization Issue**: The test comments indicate that boundary differences are used for normalization, and overflow during this process produces `NaN`\n\n## Recommended Production Code Defect\n\nThe likely defect in the `CMAESOptimizer` production code is:\n\n**Insufficient boundary range validation before normalization calculations**\n\nThe optimizer appears to be attempting to normalize variables using the difference between upper and lower bounds without first checking if this difference would cause numerical overflow. When the boundary range (upper - lower) exceeds `Double.MAX_VALUE`, the normalization calculation produces `NaN` values, which then propagate through the optimization process.\n\nSpecifically, the defect likely occurs in the boundary handling logic where:\n1. The code calculates `range = upperBound[i] - lowerBound[i]` for normalization\n2. When `upperBound[i]` and `lowerBound[i]` are both extremely large values (close to `Double.MAX_VALUE`), their difference can overflow and produce `NaN`\n3. This `NaN` value then gets passed to the fitness function\n4. The fitness function detects the `NaN` and throws `MathIllegalStateException` instead of the expected `NumberIsTooLargeException`\n\nThe production code should validate boundary ranges before performing normalization calculations and throw `NumberIsTooLargeException` when the range is too large to handle numerically, rather than allowing the overflow to occur and produce `NaN` values.",
            "test_suite": "org.apache.commons.math3.optimization.direct.CMAESOptimizerTest",
            "score": 8,
            "reason": "#8# This method is very likely to be the best location that needs to be fixed because it receives normalized parameters and handles boundary violations. The test failure occurs when extremely large boundary ranges cause numerical overflow during normalization, producing NaN values. The FitnessFunction.value() method processes these normalized points and should detect when boundary ranges are too large before attempting optimization, rather than allowing NaN values to propagate to the fitness function which then throws MathIllegalStateException instead of the expected NumberIsTooLargeException."
        }
    ],
    "buggy_codes": {
        "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::encode(double[])": {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::encode(double[])",
            "method_code": "```java\n        public double[] encode(final double[] x) {\n            if (boundaries == null) {\n                return x;\n            }\n            double[] res = new double[x.length];\n            for (int i = 0; i < x.length; i++) {\n                double diff = boundaries[1][i] - boundaries[0][i];\n                res[i] = (x[i] - boundaries[0][i]) / diff;\n            }\n            return res;\n        }\n\n```",
            "method_doc": " @param x Original objective variables. @return the normalized objective variables. "
        },
        "org.apache.commons.math3.optimization.direct.CMAESOptimizer::doOptimize()": {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer::doOptimize()",
            "method_code": "```java\n    @Override\n    protected PointValuePair doOptimize() {\n        checkParameters();\n         // -------------------- Initialization --------------------------------\n        isMinimize = getGoalType().equals(GoalType.MINIMIZE);\n        final FitnessFunction fitfun = new FitnessFunction();\n        final double[] guess = fitfun.encode(getStartPoint());\n        // number of objective variables/problem dimension\n        dimension = guess.length;\n        initializeCMA(guess);\n        iterations = 0;\n        double bestValue = fitfun.value(guess);\n        push(fitnessHistory, bestValue);\n        PointValuePair optimum = new PointValuePair(getStartPoint(),\n                isMinimize ? bestValue : -bestValue);\n        PointValuePair lastResult = null;\n\n        // -------------------- Generation Loop --------------------------------\n\n        generationLoop:\n            for (iterations = 1; iterations <= maxIterations; iterations++) {\n                // Generate and evaluate lambda offspring\n                RealMatrix arz = randn1(dimension, lambda);\n                RealMatrix arx = zeros(dimension, lambda);\n                double[] fitness = new double[lambda];\n                // generate random offspring\n                for (int k = 0; k < lambda; k++) {\n                    RealMatrix arxk = null;\n                    for (int i = 0; i < checkFeasableCount+1; i++) {\n                        if (diagonalOnly <= 0) {\n                            arxk = xmean.add(BD.multiply(arz.getColumnMatrix(k))\n                                    .scalarMultiply(sigma)); // m + sig * Normal(0,C)\n                        } else {\n                            arxk = xmean.add(times(diagD,arz.getColumnMatrix(k))\n                                    .scalarMultiply(sigma));\n                        }\n                        if (i >= checkFeasableCount || fitfun.isFeasible(arxk.getColumn(0))) {\n                            break;\n                        }\n                        // regenerate random arguments for row\n                        arz.setColumn(k, randn(dimension));\n                    }\n                    copyColumn(arxk, 0, arx, k);\n                    try {\n                        fitness[k] = fitfun.value(arx.getColumn(k)); // compute fitness\n                    } catch (TooManyEvaluationsException e) {\n                        break generationLoop;\n                    }\n                }\n                // Sort by fitness and compute weighted mean into xmean\n                int[] arindex = sortedIndices(fitness);\n                // Calculate new xmean, this is selection and recombination\n                RealMatrix xold = xmean; // for speed up of Eq. (2) and (3)\n                RealMatrix bestArx = selectColumns(arx, MathArrays.copyOf(arindex, mu));\n                xmean = bestArx.multiply(weights);\n                RealMatrix bestArz = selectColumns(arz, MathArrays.copyOf(arindex, mu));\n                RealMatrix zmean = bestArz.multiply(weights);\n                boolean hsig = updateEvolutionPaths(zmean, xold);\n                if (diagonalOnly <= 0) {\n                    updateCovariance(hsig, bestArx, arz, arindex, xold);\n                } else {\n                    updateCovarianceDiagonalOnly(hsig, bestArz, xold);\n                }\n                // Adapt step size sigma - Eq. (5)\n                sigma *= Math.exp(Math.min(1.0,(normps/chiN - 1.)*cs/damps));\n                double bestFitness = fitness[arindex[0]];\n                double worstFitness = fitness[arindex[arindex.length-1]];\n                if (bestValue > bestFitness) {\n                    bestValue = bestFitness;\n                    lastResult = optimum;\n                    optimum = new PointValuePair(\n                            fitfun.repairAndDecode(bestArx.getColumn(0)),\n                            isMinimize ? bestFitness : -bestFitness);\n                    if (getConvergenceChecker() != null && lastResult != null) {\n                        if (getConvergenceChecker().converged(iterations, optimum, lastResult)) {\n                            break generationLoop;\n                        }\n                    }\n                }\n                // handle termination criteria\n                // Break, if fitness is good enough\n                if (stopFitness != 0) { // only if stopFitness is defined\n                    if (bestFitness < (isMinimize ? stopFitness : -stopFitness)) {\n                        break generationLoop;\n                    }\n                }\n                double[] sqrtDiagC = sqrt(diagC).getColumn(0);\n                double[] pcCol = pc.getColumn(0);\n                for (int i = 0; i < dimension; i++) {\n                    if (sigma*(Math.max(Math.abs(pcCol[i]), sqrtDiagC[i])) > stopTolX) {\n                        break;\n                    }\n                    if (i >= dimension-1) {\n                        break generationLoop;\n                    }\n                }\n                for (int i = 0; i < dimension; i++) {\n                    if (sigma*sqrtDiagC[i] > stopTolUpX) {\n                        break generationLoop;\n                    }\n                }\n                double historyBest = min(fitnessHistory);\n                double historyWorst = max(fitnessHistory);\n                if (iterations > 2 && Math.max(historyWorst, worstFitness) -\n                        Math.min(historyBest, bestFitness) < stopTolFun) {\n                    break generationLoop;\n                }\n                if (iterations > fitnessHistory.length &&\n                        historyWorst-historyBest < stopTolHistFun) {\n                    break generationLoop;\n                }\n                // condition number of the covariance matrix exceeds 1e14\n                if (max(diagD)/min(diagD) > 1e7) {\n                    break generationLoop;\n                }\n                // user defined termination\n                if (getConvergenceChecker() != null) {\n                    PointValuePair current =\n                        new PointValuePair(bestArx.getColumn(0),\n                                isMinimize ? bestFitness : -bestFitness);\n                    if (lastResult != null &&\n                        getConvergenceChecker().converged(iterations, current, lastResult)) {\n                        break generationLoop;\n                    }\n                    lastResult = current;\n                }\n                // Adjust step size in case of equal function values (flat fitness)\n                if (bestValue == fitness[arindex[(int)(0.1+lambda/4.)]]) {\n                    sigma = sigma * Math.exp(0.2+cs/damps);\n                }\n                if (iterations > 2 && Math.max(historyWorst, bestFitness) -\n                        Math.min(historyBest, bestFitness) == 0) {\n                    sigma = sigma * Math.exp(0.2+cs/damps);\n                }\n                // store best in history\n                push(fitnessHistory,bestFitness);\n                fitfun.setValueRange(worstFitness-bestFitness);\n                if (generateStatistics) {\n                    statisticsSigmaHistory.add(sigma);\n                    statisticsFitnessHistory.add(bestFitness);\n                    statisticsMeanHistory.add(xmean.transpose());\n                    statisticsDHistory.add(diagD.transpose().scalarMultiply(1E5));\n                }\n            }\n        return optimum;\n    }\n\n```",
            "method_doc": "{@inheritDoc} */"
        },
        "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::decode(double[])": {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::decode(double[])",
            "method_code": "```java\n        public double[] decode(final double[] x) {\n            if (boundaries == null) {\n                return x;\n            }\n            double[] res = new double[x.length];\n            for (int i = 0; i < x.length; i++) {\n                double diff = boundaries[1][i] - boundaries[0][i];\n                res[i] = diff * x[i] + boundaries[0][i];\n            }\n            return res;\n        }\n\n```",
            "method_doc": " @param x Normalized objective variables. @return the original objective variables. "
        },
        "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::value(double[])": {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer$FitnessFunction::value(double[])",
            "method_code": "```java\n        public double value(final double[] point) {\n            double value;\n            if (boundaries != null && isRepairMode) {\n                double[] repaired = repair(point);\n                value = CMAESOptimizer.this\n                        .computeObjectiveValue(decode(repaired)) +\n                        penalty(point, repaired);\n            } else {\n                value = CMAESOptimizer.this\n                        .computeObjectiveValue(decode(point));\n            }\n            return isMinimize ? value : -value;\n        }\n\n```",
            "method_doc": " @param point Normalized objective variables. @return the objective value + penalty for violated bounds. "
        },
        "org.apache.commons.math3.optimization.direct.CMAESOptimizer::checkParameters()": {
            "method_name": "org.apache.commons.math3.optimization.direct.CMAESOptimizer::checkParameters()",
            "method_code": "```java\n    private void checkParameters() {\n        if (!(!(false))) {\n            System.out.println(\"log\");\n        }\n        final double[] TransVarFinalInit = getStartPoint();\n        final double[] TransVarLowerBound = getLowerBound();\n        final double[] TransVarUB2 = getUpperBound();\n\n        // Checks whether there is at least one finite bound value.\n        boolean TransVarHasLowerBounds = false;\n        int TransVarSum = 0;\n        while (!(!(TransVarSum < TransVarLowerBound.length))) {\n            if (!(!(false))) {\n                System.out.println(\"log\");\n            }\n            if (!(!(!Double.isInfinite(TransVarLowerBound[TransVarSum]) || !Double.isInfinite(TransVarUB2[TransVarSum])))) {\n                if (!(!(false))) {\n                    System.out.println(\"log\");\n                }\n                TransVarHasLowerBounds = true;\n                break;\n            }\n            TransVarSum++;\n        }\n        // Checks whether there is at least one infinite bound value.\n        boolean TransVarHasUpperBounds = false;\n        if (!(!(TransVarHasLowerBounds))) {\n            int TransVarInit = 0;\n            while (!(!(TransVarInit < TransVarLowerBound.length))) {\n                if (!(!(Double.isInfinite(TransVarLowerBound[TransVarInit])\n                        || Double.isInfinite(TransVarUB2[TransVarInit])))) {\n                    TransVarHasUpperBounds = true;\n                    break;\n                }\n                TransVarInit++;\n            }\n\n            if (!(!(TransVarHasUpperBounds))) {\n                // If there is at least one finite bound, none can be infinite,\n                // because mixed cases are not supported by the current code.\n                throw new MathUnsupportedOperationException();\n            } else {\n                // Convert API to internal handling of boundaries.\n                boundaries = new double[2][];\n                boundaries[0] = TransVarLowerBound;\n                boundaries[1] = TransVarUB2;\n\n                // Abort early if the normalization will overflow (cf. \"encode\" method).\n                    }\n        } else {\n            // Convert API to internal handling of boundaries.\n            boundaries = null;\n        }\n\n        if (!(!(inputSigma != null))) {\n            if (!(!(inputSigma.length != TransVarFinalInit.length))) {\n                throw new DimensionMismatchException(inputSigma.length, TransVarFinalInit.length);\n            }\n            int TransVarJ = 0;\n            while (!(!(TransVarJ < TransVarFinalInit.length))) {\n                if (!(!(inputSigma[TransVarJ] < 0))) {\n                    throw new NotPositiveException(inputSigma[TransVarJ]);\n                }\n                if (!(!(boundaries != null))) {\n                    if (!(!(inputSigma[TransVarJ] > boundaries[1][TransVarJ] - boundaries[0][TransVarJ]))) {\n                        throw new OutOfRangeException(inputSigma[TransVarJ], 0,\n                                boundaries[1][TransVarJ] - boundaries[0][TransVarJ]);\n                    }\n                }\n                TransVarJ++;\n            }\n        }\n    }\n\n```",
            "method_doc": " Checks dimensions and values of boundaries and inputSigma if defined. "
        }
    }
}