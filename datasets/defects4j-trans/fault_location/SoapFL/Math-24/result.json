{
    "buggy_classes": [
        "org.apache.commons.math3.optimization.univariate.BrentOptimizer"
    ],
    "buggy_methods": [
        {
            "method_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer::doOptimize()",
            "method_code": "```java\n    @Override\n    protected UnivariatePointValuePair doOptimize() {\n        if (!(!(false))) {\n            System.out.println(\"log\");\n        }\n        final boolean TransVarIsMinimize = getGoalType() == GoalType.MINIMIZE;\n        final double TransVarMin = getMin();\n        final double TransVarMidValue = getStartValue();\n        final double TransVarValue = getMax();\n\n        // Optional additional convergence criteria.\n        final ConvergenceChecker<UnivariatePointValuePair> TransVarConvergenceChecker\n            = getConvergenceChecker();\n\n        double TransVarB;\n        double TransVarBest;\n        if (!(!(TransVarMin < TransVarValue))) {\n            if (!(!(false))) {\n                System.out.println(\"log\");\n            }\n            TransVarB = TransVarMin;\n            TransVarBest = TransVarValue;\n        } else {\n            if (!(!(false))) {\n                System.out.println(\"log\");\n        }\n            TransVarB = TransVarValue;\n            TransVarBest = TransVarMin;\n        }\n\n        double TransVarMid = TransVarMidValue;\n        double TransVarVariable = TransVarMid;\n        double TransVarW1 = TransVarMid;\n        double TransVarD1 = 0;\n        double TransVarError = 0;\n        double TransVarF = computeObjectiveValue(TransVarMid);\n        if (!(!(!TransVarIsMinimize))) {\n            TransVarF = -TransVarF;\n        }\n        double TransVarValueFunction = TransVarF;\n        double TransVarFxw = TransVarF;\n\n        UnivariatePointValuePair TransVarPreviousPointValuePair = null;\n        UnivariatePointValuePair TransVarCurrentPoint\n            = new UnivariatePointValuePair(TransVarMid, TransVarIsMinimize ? TransVarF : -TransVarF);\n\n        int TransVarI = 0;\n        for (; !(!(true));) {\n            final double TransVarM2 = 0.5 * (TransVarB + TransVarBest);\n            final double TransVarTol2 = relativeThreshold * FastMath.abs(TransVarMid) + absoluteThreshold;\n            final double TransVarTol1 = 2 * TransVarTol2;\n            final boolean TransVarIsStop = FastMath.abs(TransVarMid - TransVarM2) <= TransVarTol1\n                    - 0.5 * (TransVarBest - TransVarB);\n            if (!(!(!TransVarIsStop))) {\n                double TransVarP1 = 0;\n                double TransVarQ1 = 0;\n                double TransVarR1 = 0;\n                double TransVarInitialValue = 0;\n                if (!(!(FastMath.abs(TransVarError) > TransVarTol2))) {\n                    TransVarR1 = (TransVarMid - TransVarW1) * (TransVarF - TransVarValueFunction);\n                    TransVarQ1 = (TransVarMid - TransVarVariable) * (TransVarF - TransVarFxw);\n                    TransVarP1 = (TransVarMid - TransVarVariable) * TransVarQ1\n                            - (TransVarMid - TransVarW1) * TransVarR1;\n                    TransVarQ1 = 2 * (TransVarQ1 - TransVarR1);\n                    if (!(!(TransVarQ1 > 0))) {\n                        TransVarP1 = -TransVarP1;\n                    } else {\n                        TransVarQ1 = -TransVarQ1;\n                    }\n                    TransVarR1 = TransVarError;\n                    TransVarError = TransVarD1;\n                    if (!(!(TransVarP1 > TransVarQ1 * (TransVarB - TransVarMid)\n                            && TransVarP1 < TransVarQ1 * (TransVarBest - TransVarMid)\n                            && FastMath.abs(TransVarP1) < FastMath.abs(0.5 * TransVarQ1 * TransVarR1)))) {\n                        TransVarD1 = TransVarP1 / TransVarQ1;\n                        TransVarInitialValue = TransVarMid + TransVarD1;\n                        if (!(!(TransVarInitialValue - TransVarB < TransVarTol1\n                                || TransVarBest - TransVarInitialValue < TransVarTol1))) {\n                            if (!(!(TransVarMid <= TransVarM2))) {\n                                TransVarD1 = TransVarTol2;\n                            } else {\n                                TransVarD1 = -TransVarTol2;\n                            }\n                        }\n                    } else {\n                        if (!(!(TransVarMid < TransVarM2))) {\n                            TransVarError = TransVarBest - TransVarMid;\n                        } else {\n                            TransVarError = TransVarB - TransVarMid;\n                        }\n                        TransVarD1 = GOLDEN_SECTION * TransVarError;\n                    }\n                } else {\n                    if (!(!(TransVarMid < TransVarM2))) {\n                        TransVarError = TransVarBest - TransVarMid;\n                    } else {\n                        TransVarError = TransVarB - TransVarMid;\n                    }\n                    TransVarD1 = GOLDEN_SECTION * TransVarError;\n                }\n                if (!(!(FastMath.abs(TransVarD1) < TransVarTol2))) {\n                    if (!(!(TransVarD1 >= 0))) {\n                        TransVarInitialValue = TransVarMid + TransVarTol2;\n                    } else {\n                        TransVarInitialValue = TransVarMid - TransVarTol2;\n                    }\n                } else {\n                    TransVarInitialValue = TransVarMid + TransVarD1;\n                }\n                double TransVarObjVal = computeObjectiveValue(TransVarInitialValue);\n                if (!(!(!TransVarIsMinimize))) {\n                    TransVarObjVal = -TransVarObjVal;\n                }\n                TransVarPreviousPointValuePair = TransVarCurrentPoint;\n                TransVarCurrentPoint = new UnivariatePointValuePair(TransVarInitialValue,\n                        TransVarIsMinimize ? TransVarObjVal : -TransVarObjVal);\n                if (!(!(TransVarConvergenceChecker != null))) {\n                    if (!(!(TransVarConvergenceChecker.converged(TransVarI, TransVarPreviousPointValuePair,\n                            TransVarCurrentPoint)))) {\n                        return TransVarCurrentPoint;\n                    }\n                }\n                if (!(!(TransVarObjVal <= TransVarF))) {\n                    if (!(!(TransVarInitialValue < TransVarMid))) {\n                        TransVarBest = TransVarMid;\n                    } else {\n                        TransVarB = TransVarMid;\n                    }\n                    TransVarVariable = TransVarW1;\n                    TransVarValueFunction = TransVarFxw;\n                    TransVarW1 = TransVarMid;\n                    TransVarFxw = TransVarF;\n                    TransVarMid = TransVarInitialValue;\n                    TransVarF = TransVarObjVal;\n                } else {\n                    if (!(!(TransVarInitialValue < TransVarMid))) {\n                        TransVarB = TransVarInitialValue;\n                    } else {\n                        TransVarBest = TransVarInitialValue;\n                    }\n                    if (!(!(TransVarObjVal <= TransVarFxw || Precision.equals(TransVarW1, TransVarMid)))) {\n                        TransVarVariable = TransVarW1;\n                        TransVarValueFunction = TransVarFxw;\n                        TransVarW1 = TransVarInitialValue;\n                        TransVarFxw = TransVarObjVal;\n                    } else if (!(!(TransVarObjVal <= TransVarValueFunction || Precision.equals(TransVarVariable, TransVarMid)\n                            || Precision.equals(TransVarVariable, TransVarW1)))) {\n                        TransVarVariable = TransVarInitialValue;\n                        TransVarValueFunction = TransVarObjVal;\n                    }\n                }\n            } else {\n                return TransVarCurrentPoint;\n            }\n            ++TransVarI;\n        }\n    }\n\n```",
            "method_doc": "{@inheritDoc} */",
            "class_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer",
            "class_doc": " Implements Richard Brent's algorithm (from his book \"Algorithms for Minimization without Derivatives\", p. 79) for finding minima of real univariate functions. This implementation is an adaptation partly based on the Python code from SciPy (module \"optimize.py\" v0.5). If the function is defined on some interval {@code (lo, hi)}, then this method finds an approximation {@code x} to the point at which the function attains its minimum.  @version $Id$ @since 2.0 ",
            "test_failure_causes": "As Software Test Engineer at DebugDev, I'll analyze this test failure systematically to identify the root cause in the production code.\n\n## Analysis of Test Failure Patterns\n\n**Common Pattern Identified:**\nThe test `testMath855` specifically addresses an issue where the BrentOptimizer algorithm fails to report the best point found during optimization, instead returning the last evaluated point. This is a regression test for MATH-855, indicating this was a known historical defect.\n\n## Root Cause Analysis\n\nBased on the test behavior and failure pattern, I identify the following potential defects in the **BrentOptimizer production code**:\n\n### Primary Defect Hypothesis:\n**Incorrect Point Selection Logic in BrentOptimizer**\n\nThe optimizer appears to have a flaw in how it tracks and returns the best solution found during the optimization process. Specifically:\n\n1. **Best Point Tracking Failure**: The algorithm may not be properly comparing and storing the best (minimum) function value encountered during iterations\n2. **Final Point Preference**: The implementation might be incorrectly favoring the last evaluated point over the actual best point found\n3. **Convergence Logic Error**: The termination condition may be checking convergence against the current iteration point rather than the historically best point\n\n### Supporting Evidence from Test Analysis:\n\n- The test creates a function with a sharp \"dip\" (via StepFunction) that creates a very localized minimum\n- With tight tolerances (1e-8 relative, 1e-100 absolute), the optimizer should find this minimum\n- The assertion failure indicates the returned solution has a higher function value than the expected optimal point\n- This suggests the optimizer evaluated the optimal point during its search but failed to return it\n\n### Recommended Investigation Areas in Production Code:\n\n1. **Check the point comparison logic** in the main optimization loop - ensure it properly tracks the absolute best point encountered\n2. **Verify the solution return mechanism** - the algorithm should return the historically best point, not just the final point\n3. **Examine convergence criteria** - ensure termination decisions don't override best-point tracking\n4. **Review the golden section search implementation** within Brent's method for proper best-value retention\n\nThe defect appears to be in the core optimization logic where the algorithm loses track of the true minimum found during the search process, particularly when dealing with functions that have sharp, localized features.",
            "test_suite": "org.apache.commons.math3.optimization.univariate.BrentOptimizerTest",
            "score": 9,
            "reason": "#9# This method is highly suspicious because it contains flawed logic for tracking the best point found during optimization. The test failure specifically indicates that the optimizer returns a suboptimal point instead of the true minimum, which suggests the algorithm is not properly comparing and storing the historically best function value. The complex conditional logic and point assignment patterns in the optimization loop appear to incorrectly handle the selection between the current point and previously evaluated points, particularly when dealing with functions that have sharp local minima like in testMath855."
        },
        {
            "method_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer::BrentOptimizer(double,double)",
            "method_code": "```java\n    public BrentOptimizer(double rel,\n                          double abs) {\n        this(rel, abs, null);\n    }\n\n```",
            "method_doc": " The arguments are used for implementing the original stopping criterion of Brent's algorithm. {@code abs} and {@code rel} define a tolerance {@code tol = rel |x| + abs}. {@code rel} should be no smaller than <em>2 macheps</em> and preferably not much less than <em>sqrt(macheps)</em>, where <em>macheps</em> is the relative machine precision. {@code abs} must be positive.  @param rel Relative threshold. @param abs Absolute threshold. @throws NotStrictlyPositiveException if {@code abs <= 0}. @throws NumberIsTooSmallException if {@code rel < 2 * Math.ulp(1d)}. ",
            "class_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer",
            "class_doc": " Implements Richard Brent's algorithm (from his book \"Algorithms for Minimization without Derivatives\", p. 79) for finding minima of real univariate functions. This implementation is an adaptation partly based on the Python code from SciPy (module \"optimize.py\" v0.5). If the function is defined on some interval {@code (lo, hi)}, then this method finds an approximation {@code x} to the point at which the function attains its minimum.  @version $Id$ @since 2.0 ",
            "test_failure_causes": "As Software Test Engineer at DebugDev, I'll analyze this test failure systematically to identify the root cause in the production code.\n\n## Analysis of Test Failure Patterns\n\n**Common Pattern Identified:**\nThe test `testMath855` specifically addresses an issue where the BrentOptimizer algorithm fails to report the best point found during optimization, instead returning the last evaluated point. This is a regression test for MATH-855, indicating this was a known historical defect.\n\n## Root Cause Analysis\n\nBased on the test behavior and failure pattern, I identify the following potential defects in the **BrentOptimizer production code**:\n\n### Primary Defect Hypothesis:\n**Incorrect Point Selection Logic in BrentOptimizer**\n\nThe optimizer appears to have a flaw in how it tracks and returns the best solution found during the optimization process. Specifically:\n\n1. **Best Point Tracking Failure**: The algorithm may not be properly comparing and storing the best (minimum) function value encountered during iterations\n2. **Final Point Preference**: The implementation might be incorrectly favoring the last evaluated point over the actual best point found\n3. **Convergence Logic Error**: The termination condition may be checking convergence against the current iteration point rather than the historically best point\n\n### Supporting Evidence from Test Analysis:\n\n- The test creates a function with a sharp \"dip\" (via StepFunction) that creates a very localized minimum\n- With tight tolerances (1e-8 relative, 1e-100 absolute), the optimizer should find this minimum\n- The assertion failure indicates the returned solution has a higher function value than the expected optimal point\n- This suggests the optimizer evaluated the optimal point during its search but failed to return it\n\n### Recommended Investigation Areas in Production Code:\n\n1. **Check the point comparison logic** in the main optimization loop - ensure it properly tracks the absolute best point encountered\n2. **Verify the solution return mechanism** - the algorithm should return the historically best point, not just the final point\n3. **Examine convergence criteria** - ensure termination decisions don't override best-point tracking\n4. **Review the golden section search implementation** within Brent's method for proper best-value retention\n\nThe defect appears to be in the core optimization logic where the algorithm loses track of the true minimum found during the search process, particularly when dealing with functions that have sharp, localized features.",
            "test_suite": "org.apache.commons.math3.optimization.univariate.BrentOptimizerTest",
            "score": 2,
            "reason": "#2# The constructor method only delegates to another constructor and doesn't contain any optimization logic. The actual bug is likely in the optimization algorithm implementation within the optimize() method, not in this simple constructor that just sets tolerance parameters."
        },
        {
            "method_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer::BrentOptimizer(double,double,ConvergenceChecker)",
            "method_code": "```java\n    public BrentOptimizer(double rel,\n                          double abs,\n                          ConvergenceChecker<UnivariatePointValuePair> checker) {\n        super(checker);\n\n        if (rel < MIN_RELATIVE_TOLERANCE) {\n            throw new NumberIsTooSmallException(rel, MIN_RELATIVE_TOLERANCE, true);\n        }\n        if (abs <= 0) {\n            throw new NotStrictlyPositiveException(abs);\n        }\n\n        relativeThreshold = rel;\n        absoluteThreshold = abs;\n    }\n\n```",
            "method_doc": " The arguments are used implement the original stopping criterion of Brent's algorithm. {@code abs} and {@code rel} define a tolerance {@code tol = rel |x| + abs}. {@code rel} should be no smaller than <em>2 macheps</em> and preferably not much less than <em>sqrt(macheps)</em>, where <em>macheps</em> is the relative machine precision. {@code abs} must be positive.  @param rel Relative threshold. @param abs Absolute threshold. @param checker Additional, user-defined, convergence checking procedure. @throws NotStrictlyPositiveException if {@code abs <= 0}. @throws NumberIsTooSmallException if {@code rel < 2 * Math.ulp(1d)}. ",
            "class_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer",
            "class_doc": " Implements Richard Brent's algorithm (from his book \"Algorithms for Minimization without Derivatives\", p. 79) for finding minima of real univariate functions. This implementation is an adaptation partly based on the Python code from SciPy (module \"optimize.py\" v0.5). If the function is defined on some interval {@code (lo, hi)}, then this method finds an approximation {@code x} to the point at which the function attains its minimum.  @version $Id$ @since 2.0 ",
            "test_failure_causes": "As Software Test Engineer at DebugDev, I'll analyze this test failure systematically to identify the root cause in the production code.\n\n## Analysis of Test Failure Patterns\n\n**Common Pattern Identified:**\nThe test `testMath855` specifically addresses an issue where the BrentOptimizer algorithm fails to report the best point found during optimization, instead returning the last evaluated point. This is a regression test for MATH-855, indicating this was a known historical defect.\n\n## Root Cause Analysis\n\nBased on the test behavior and failure pattern, I identify the following potential defects in the **BrentOptimizer production code**:\n\n### Primary Defect Hypothesis:\n**Incorrect Point Selection Logic in BrentOptimizer**\n\nThe optimizer appears to have a flaw in how it tracks and returns the best solution found during the optimization process. Specifically:\n\n1. **Best Point Tracking Failure**: The algorithm may not be properly comparing and storing the best (minimum) function value encountered during iterations\n2. **Final Point Preference**: The implementation might be incorrectly favoring the last evaluated point over the actual best point found\n3. **Convergence Logic Error**: The termination condition may be checking convergence against the current iteration point rather than the historically best point\n\n### Supporting Evidence from Test Analysis:\n\n- The test creates a function with a sharp \"dip\" (via StepFunction) that creates a very localized minimum\n- With tight tolerances (1e-8 relative, 1e-100 absolute), the optimizer should find this minimum\n- The assertion failure indicates the returned solution has a higher function value than the expected optimal point\n- This suggests the optimizer evaluated the optimal point during its search but failed to return it\n\n### Recommended Investigation Areas in Production Code:\n\n1. **Check the point comparison logic** in the main optimization loop - ensure it properly tracks the absolute best point encountered\n2. **Verify the solution return mechanism** - the algorithm should return the historically best point, not just the final point\n3. **Examine convergence criteria** - ensure termination decisions don't override best-point tracking\n4. **Review the golden section search implementation** within Brent's method for proper best-value retention\n\nThe defect appears to be in the core optimization logic where the algorithm loses track of the true minimum found during the search process, particularly when dealing with functions that have sharp, localized features.",
            "test_suite": "org.apache.commons.math3.optimization.univariate.BrentOptimizerTest",
            "score": 2,
            "reason": "#2# The constructor method only validates parameters and sets tolerance thresholds, it doesn't contain the core optimization logic that would cause the algorithm to return suboptimal points. The actual bug is likely in the main optimization algorithm implementation, not in this parameter validation constructor."
        }
    ],
    "buggy_codes": {
        "org.apache.commons.math3.optimization.univariate.BrentOptimizer::BrentOptimizer(double,double)": {
            "method_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer::BrentOptimizer(double,double)",
            "method_code": "```java\n    public BrentOptimizer(double rel,\n                          double abs) {\n        this(rel, abs, null);\n    }\n\n```",
            "method_doc": " The arguments are used for implementing the original stopping criterion of Brent's algorithm. {@code abs} and {@code rel} define a tolerance {@code tol = rel |x| + abs}. {@code rel} should be no smaller than <em>2 macheps</em> and preferably not much less than <em>sqrt(macheps)</em>, where <em>macheps</em> is the relative machine precision. {@code abs} must be positive.  @param rel Relative threshold. @param abs Absolute threshold. @throws NotStrictlyPositiveException if {@code abs <= 0}. @throws NumberIsTooSmallException if {@code rel < 2 * Math.ulp(1d)}. "
        },
        "org.apache.commons.math3.optimization.univariate.BrentOptimizer::doOptimize()": {
            "method_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer::doOptimize()",
            "method_code": "```java\n    @Override\n    protected UnivariatePointValuePair doOptimize() {\n        if (!(!(false))) {\n            System.out.println(\"log\");\n        }\n        final boolean TransVarIsMinimize = getGoalType() == GoalType.MINIMIZE;\n        final double TransVarMin = getMin();\n        final double TransVarMidValue = getStartValue();\n        final double TransVarValue = getMax();\n\n        // Optional additional convergence criteria.\n        final ConvergenceChecker<UnivariatePointValuePair> TransVarConvergenceChecker\n            = getConvergenceChecker();\n\n        double TransVarB;\n        double TransVarBest;\n        if (!(!(TransVarMin < TransVarValue))) {\n            if (!(!(false))) {\n                System.out.println(\"log\");\n            }\n            TransVarB = TransVarMin;\n            TransVarBest = TransVarValue;\n        } else {\n            if (!(!(false))) {\n                System.out.println(\"log\");\n        }\n            TransVarB = TransVarValue;\n            TransVarBest = TransVarMin;\n        }\n\n        double TransVarMid = TransVarMidValue;\n        double TransVarVariable = TransVarMid;\n        double TransVarW1 = TransVarMid;\n        double TransVarD1 = 0;\n        double TransVarError = 0;\n        double TransVarF = computeObjectiveValue(TransVarMid);\n        if (!(!(!TransVarIsMinimize))) {\n            TransVarF = -TransVarF;\n        }\n        double TransVarValueFunction = TransVarF;\n        double TransVarFxw = TransVarF;\n\n        UnivariatePointValuePair TransVarPreviousPointValuePair = null;\n        UnivariatePointValuePair TransVarCurrentPoint\n            = new UnivariatePointValuePair(TransVarMid, TransVarIsMinimize ? TransVarF : -TransVarF);\n\n        int TransVarI = 0;\n        for (; !(!(true));) {\n            final double TransVarM2 = 0.5 * (TransVarB + TransVarBest);\n            final double TransVarTol2 = relativeThreshold * FastMath.abs(TransVarMid) + absoluteThreshold;\n            final double TransVarTol1 = 2 * TransVarTol2;\n            final boolean TransVarIsStop = FastMath.abs(TransVarMid - TransVarM2) <= TransVarTol1\n                    - 0.5 * (TransVarBest - TransVarB);\n            if (!(!(!TransVarIsStop))) {\n                double TransVarP1 = 0;\n                double TransVarQ1 = 0;\n                double TransVarR1 = 0;\n                double TransVarInitialValue = 0;\n                if (!(!(FastMath.abs(TransVarError) > TransVarTol2))) {\n                    TransVarR1 = (TransVarMid - TransVarW1) * (TransVarF - TransVarValueFunction);\n                    TransVarQ1 = (TransVarMid - TransVarVariable) * (TransVarF - TransVarFxw);\n                    TransVarP1 = (TransVarMid - TransVarVariable) * TransVarQ1\n                            - (TransVarMid - TransVarW1) * TransVarR1;\n                    TransVarQ1 = 2 * (TransVarQ1 - TransVarR1);\n                    if (!(!(TransVarQ1 > 0))) {\n                        TransVarP1 = -TransVarP1;\n                    } else {\n                        TransVarQ1 = -TransVarQ1;\n                    }\n                    TransVarR1 = TransVarError;\n                    TransVarError = TransVarD1;\n                    if (!(!(TransVarP1 > TransVarQ1 * (TransVarB - TransVarMid)\n                            && TransVarP1 < TransVarQ1 * (TransVarBest - TransVarMid)\n                            && FastMath.abs(TransVarP1) < FastMath.abs(0.5 * TransVarQ1 * TransVarR1)))) {\n                        TransVarD1 = TransVarP1 / TransVarQ1;\n                        TransVarInitialValue = TransVarMid + TransVarD1;\n                        if (!(!(TransVarInitialValue - TransVarB < TransVarTol1\n                                || TransVarBest - TransVarInitialValue < TransVarTol1))) {\n                            if (!(!(TransVarMid <= TransVarM2))) {\n                                TransVarD1 = TransVarTol2;\n                            } else {\n                                TransVarD1 = -TransVarTol2;\n                            }\n                        }\n                    } else {\n                        if (!(!(TransVarMid < TransVarM2))) {\n                            TransVarError = TransVarBest - TransVarMid;\n                        } else {\n                            TransVarError = TransVarB - TransVarMid;\n                        }\n                        TransVarD1 = GOLDEN_SECTION * TransVarError;\n                    }\n                } else {\n                    if (!(!(TransVarMid < TransVarM2))) {\n                        TransVarError = TransVarBest - TransVarMid;\n                    } else {\n                        TransVarError = TransVarB - TransVarMid;\n                    }\n                    TransVarD1 = GOLDEN_SECTION * TransVarError;\n                }\n                if (!(!(FastMath.abs(TransVarD1) < TransVarTol2))) {\n                    if (!(!(TransVarD1 >= 0))) {\n                        TransVarInitialValue = TransVarMid + TransVarTol2;\n                    } else {\n                        TransVarInitialValue = TransVarMid - TransVarTol2;\n                    }\n                } else {\n                    TransVarInitialValue = TransVarMid + TransVarD1;\n                }\n                double TransVarObjVal = computeObjectiveValue(TransVarInitialValue);\n                if (!(!(!TransVarIsMinimize))) {\n                    TransVarObjVal = -TransVarObjVal;\n                }\n                TransVarPreviousPointValuePair = TransVarCurrentPoint;\n                TransVarCurrentPoint = new UnivariatePointValuePair(TransVarInitialValue,\n                        TransVarIsMinimize ? TransVarObjVal : -TransVarObjVal);\n                if (!(!(TransVarConvergenceChecker != null))) {\n                    if (!(!(TransVarConvergenceChecker.converged(TransVarI, TransVarPreviousPointValuePair,\n                            TransVarCurrentPoint)))) {\n                        return TransVarCurrentPoint;\n                    }\n                }\n                if (!(!(TransVarObjVal <= TransVarF))) {\n                    if (!(!(TransVarInitialValue < TransVarMid))) {\n                        TransVarBest = TransVarMid;\n                    } else {\n                        TransVarB = TransVarMid;\n                    }\n                    TransVarVariable = TransVarW1;\n                    TransVarValueFunction = TransVarFxw;\n                    TransVarW1 = TransVarMid;\n                    TransVarFxw = TransVarF;\n                    TransVarMid = TransVarInitialValue;\n                    TransVarF = TransVarObjVal;\n                } else {\n                    if (!(!(TransVarInitialValue < TransVarMid))) {\n                        TransVarB = TransVarInitialValue;\n                    } else {\n                        TransVarBest = TransVarInitialValue;\n                    }\n                    if (!(!(TransVarObjVal <= TransVarFxw || Precision.equals(TransVarW1, TransVarMid)))) {\n                        TransVarVariable = TransVarW1;\n                        TransVarValueFunction = TransVarFxw;\n                        TransVarW1 = TransVarInitialValue;\n                        TransVarFxw = TransVarObjVal;\n                    } else if (!(!(TransVarObjVal <= TransVarValueFunction || Precision.equals(TransVarVariable, TransVarMid)\n                            || Precision.equals(TransVarVariable, TransVarW1)))) {\n                        TransVarVariable = TransVarInitialValue;\n                        TransVarValueFunction = TransVarObjVal;\n                    }\n                }\n            } else {\n                return TransVarCurrentPoint;\n            }\n            ++TransVarI;\n        }\n    }\n\n```",
            "method_doc": "{@inheritDoc} */"
        },
        "org.apache.commons.math3.optimization.univariate.BrentOptimizer::BrentOptimizer(double,double,ConvergenceChecker)": {
            "method_name": "org.apache.commons.math3.optimization.univariate.BrentOptimizer::BrentOptimizer(double,double,ConvergenceChecker)",
            "method_code": "```java\n    public BrentOptimizer(double rel,\n                          double abs,\n                          ConvergenceChecker<UnivariatePointValuePair> checker) {\n        super(checker);\n\n        if (rel < MIN_RELATIVE_TOLERANCE) {\n            throw new NumberIsTooSmallException(rel, MIN_RELATIVE_TOLERANCE, true);\n        }\n        if (abs <= 0) {\n            throw new NotStrictlyPositiveException(abs);\n        }\n\n        relativeThreshold = rel;\n        absoluteThreshold = abs;\n    }\n\n```",
            "method_doc": " The arguments are used implement the original stopping criterion of Brent's algorithm. {@code abs} and {@code rel} define a tolerance {@code tol = rel |x| + abs}. {@code rel} should be no smaller than <em>2 macheps</em> and preferably not much less than <em>sqrt(macheps)</em>, where <em>macheps</em> is the relative machine precision. {@code abs} must be positive.  @param rel Relative threshold. @param abs Absolute threshold. @param checker Additional, user-defined, convergence checking procedure. @throws NotStrictlyPositiveException if {@code abs <= 0}. @throws NumberIsTooSmallException if {@code rel < 2 * Math.ulp(1d)}. "
        }
    }
}